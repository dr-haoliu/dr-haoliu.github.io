<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Steer-Prediction from Camera Images for Self Driving Car | Hao Liu</title> <meta name="author" content="Hao Liu"> <meta name="description" content="NJIT CS782 Pattern Recognition &amp; Applications Course Project"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/al-folio/assets/css/main.css"> <link rel="canonical" href="https://dr-haoliu.github.io/al-folio/projects/4_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/al-folio/assets/js/theme.js"></script> <script src="/al-folio/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/al-folio/"><span class="font-weight-bold">Hao </span><span class="font-weight-bold">Liu</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/al-folio/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/cv/">Curriculum Vitae</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/news/">News</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Steer-Prediction from Camera Images for Self Driving Car</h1> <p class="post-description">NJIT CS782 Pattern Recognition &amp; Applications Course Project</p> </header> <article> <p>Source reference: <a href="https://www.udacity.com/nanodegree" rel="external nofollow noopener" target="_blank">Udacity’s Self-Driving Car Nanodegree project 3 - Behavioural Cloning</a></p> <h2 id="abstract">Abstract</h2> <p>The goal of this project is to train a deep learning model to predict steering angle for autonomous driving car in a simulator provided by Udacity. Using the vehicle’s camera images captured from the human driving demonstration, we train a deep neural network to predict the vehicle’s steering angle. The final trained model is evaluated both on the training track and an unseen testing track.</p> <h2 id="overview">Overview</h2> <p>The project is consisted of the following modules:</p> <ul> <li>Project Requirement</li> <li>Setup and Environment</li> <li>Exploring the data</li> <li>Data Augmentation</li> <li>Deep Learning Model Design</li> <li>Model Training</li> <li>Performance &amp; Evaluation</li> <li>Discussion</li> <li>Future Work</li> <li>Reference</li> </ul> <h2 id="project-requirement">Project Requirement</h2> <p>Deep learning model is trained only on <strong>Track 1</strong> data. To assess the trained model’s performance, the car has to successfully drive by itself without getting off the road on <strong>Track 1</strong> and drive the entire <strong>Track 2</strong> without getting off the road.</p> <p><strong>Track 1</strong>: <em>flat road, mostly straight driving, occasionally sharp turns, bright day light.</em></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/track1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/track1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/track1-1400.webp"></source> <img src="/al-folio/assets/img/project_4/track1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Track 2</strong>: <em>hilly road, many light turns and sharp turns, dark environment, heavy shadows</em></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/track2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/track2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/track2-1400.webp"></source> <img src="/al-folio/assets/img/project_4/track2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="setup-and-environment">Setup and Environment</h2> <h3 id="installation--resources">Installation &amp; Resources</h3> <ol> <li>Python 3.5</li> <li><a href="https://anaconda.org/hl395/autodrive35/2017.11.01.1933/download/autodrive35.yml" rel="external nofollow noopener" target="_blank">Anaconda Environment</a></li> <li>Udacity Car Simulator on <a href="https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f0f7_simulator-linux/simulator-linux.zip" rel="external nofollow noopener" target="_blank">Linux</a> </li> <li>Udacity <a href="https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip" rel="external nofollow noopener" target="_blank">sample data</a> </li> <li>Jupyter Notebook <a href="http://jupyter.org/" rel="external nofollow noopener" target="_blank">Download</a> Both model training and drive programs are divided into steps with staged output and detailed explanation.</li> </ol> <h3 id="files-and-usage">Files and Usage</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">autodrive35.yml</code>: Anaconda Environment configuration file.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">p3-behavioural-cloning.ipynb</code> : training program.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">model.json</code>: saved training model.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">model.h5</code>: saved training weights.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">drive.ipynb</code>: program that takes <code class="language-plaintext highlighter-rouge">model.json</code> and <code class="language-plaintext highlighter-rouge">model.h5</code> as input to excute.</p> </li> <li> <p><strong>To use</strong>:</p> <ul> <li>Start Simulator, pick track and choose Autonomous mode</li> <li>On <code class="language-plaintext highlighter-rouge">drive.ipynb</code>, specify path to <code class="language-plaintext highlighter-rouge">model.json</code> and <code class="language-plaintext highlighter-rouge">model.h5</code> </li> <li>Run cell in <code class="language-plaintext highlighter-rouge">drive.ipynb</code> </li> </ul> </li> </ul> <h3 id="quickstart">Quickstart</h3> <ul> <li> <p><strong>1. Control of the car is by using button on PC keyboard or joystick or game controller:</strong></p> <p><img class="emoji" title=":arrow_up:" alt=":arrow_up:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b06.png" height="20" width="20"> accelerate <img class="emoji" title=":arrow_down:" alt=":arrow_down:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b07.png" height="20" width="20"> brake <img class="emoji" title=":arrow_left:" alt=":arrow_left:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b05.png" height="20" width="20"> steer left <img class="emoji" title=":arrow_right:" alt=":arrow_right:" src="https://github.githubassets.com/images/icons/emoji/unicode/27a1.png" height="20" width="20"> steer right</p> </li> <li> <strong>2. Two driving modes:</strong> <ul> <li>Training: User demonstrates driving on track</li> <li>Autonomous: Car drives itself by receiving commands from program</li> </ul> </li> <li> <strong>3. Collecting data:</strong> <ul> <li>User drives on track 1 and collects data by recording the driving experience by toggle ON/OFF the recorder. Data is saved as frame images and a driving log which shows the location of the images, steering angle, throttle, speed, etc. Training images were sampled at 10 Hz.</li> <li>Another option is trying on Udacity data sample.</li> </ul> </li> </ul> <h2 id="exploring-the-data">Exploring the data</h2> <h3 id="data-formatcomponent">Data Format/Component</h3> <ul> <li> <strong>Camera Frame:</strong> <ul> <li>There are 3 cameras on the car which shows left, center and right images for each steering angle.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/camera_frame-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/camera_frame-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/camera_frame-1400.webp"></source> <img src="/al-folio/assets/img/project_4/camera_frame.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Driving Log:</strong> <ul> <li>After recording and save data, the simulator saves all the frame images in <code class="language-plaintext highlighter-rouge">IMG</code> folder and produces a <code class="language-plaintext highlighter-rouge">driving_log.csv</code> file which containts all the information needed for data preparation such as path to images folder, steering angle at each frame, throttle, brake and speed values.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/drive_log-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/drive_log-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/drive_log-1400.webp"></source> <img src="/al-folio/assets/img/project_4/drive_log.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In this project, we only need to predict steering angle. So we will ignore throttle, brake and speed information.</p> <h3 id="smooth-steering-angle">Smooth Steering Angle</h3> <p>Since driving data was collected using keyboard inputs, the input time-series is very choppy. To smooth the data, I used a simple moving average, with a window of 3 time units (i.e. 0.3 seconds). Below is a plot of the raw and smoothed steering angle over time, for a random section in my normal driving data:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/smooth-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/smooth-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/smooth-1400.webp"></source> <img src="/al-folio/assets/img/project_4/smooth.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="data-balancing">Data Balancing</h3> <p>Collected data is not balanced, we can see the steering angle historgram as shown below and data balancing is a crucial step for network to have good performance!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/samples_hist1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/samples_hist1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/samples_hist1-1400.webp"></source> <img src="/al-folio/assets/img/project_4/samples_hist1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In order to balance the data, first, we can draw a histogram to see which steering angle ranges are most dominating in the sample space(high bins in the drawing). Secondly, we can calculate average samples per bin(as <strong>average_samples_per_bin</strong>) by dividing the total number of samples by the number of bins(200). Thirdly, we can determine keep probability for each bin: if the number of samples in a bin is below <strong>average_samples_per_bin</strong>, keep all; otherwise, the keep probability is set proportional to the number of samples above the average, in order to bring the number of samples for that bin down to the average.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/samples_hist2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/samples_hist2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/samples_hist2-1400.webp"></source> <img src="/al-folio/assets/img/project_4/samples_hist2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We can see from above figure, still, most of the steer angles are around the center(0 degree). There is very low probability that we will pick up the large angle portion samples. Thus, we need to further reduce the number of high bins, and this is done by function <code class="language-plaintext highlighter-rouge">balance_data()</code>. I use this function to bring the number of samples in each bin to 100 at most among 100 bins. After these steps, I had 4417 number of data points. The results are shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/hist-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/hist-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/hist-1400.webp"></source> <img src="/al-folio/assets/img/project_4/hist.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="data-augmentation">Data Augmentation</h2> <p>Apparently, 4417 samples is a small training set, but we can apply some tricks that are already widly used for image classification to generate more samples:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/augmentation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/augmentation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/augmentation-1400.webp"></source> <img src="/al-folio/assets/img/project_4/augmentation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>reference: <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" rel="external nofollow noopener" target="_blank">Building powerful image classification models using very little data</a></p> <p>Here I chose 4 methods which are relevant to this project:</p> <ul> <li> <strong>Image Flipping</strong>: In track 1, most of the turns are left turns, so I flipped images and angles. As a result, the network would learn both left and right turns properly. Here are two images that have then been flipped:</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/flip-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/flip-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/flip-1400.webp"></source> <img src="/al-folio/assets/img/project_4/flip.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Brightness Augmentation</strong>: In order to learn a more general model, I randomly changed the image’s brightness in HSV space(function <em>brightness_change</em>):</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/brightness-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/brightness-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/brightness-1400.webp"></source> <img src="/al-folio/assets/img/project_4/brightness.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Shadow augmentation</strong>: To deal with tree, building or other object shadow on the road, random shadows are cast across the image. This is implemented by choosing random points and shading all points on one side (chosen randomly) of the image(function <em>add_random_shadow</em>):</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/shadow-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/shadow-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/shadow-1400.webp"></source> <img src="/al-folio/assets/img/project_4/shadow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Horizontal and vertical shifts</strong>: To simulate the effect of car being at different positions (driving up or down the slope) on the road, we can shift the camera images horizontally (vertically) and add an offset corresponding to the steering angle. We added 0.004 steering angle units per pixel shift to the right, and subtracted 0.004 steering angle units per pixel shift to the left(function <em>trans_image</em>):</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/shifted-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/shifted-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/shifted-1400.webp"></source> <img src="/al-folio/assets/img/project_4/shifted.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Note: I only use data augmentation in training phase, thus for each data point(sample), I will select FOUR images: center, left, right and horizontal shifting on center image. Then these four images then go through data augmentation process: Image Flipping, Brightness Augmentation and Shadow Augmentation. In validation phase, only center images with simulator provided angle labels are used to guarantee accuracy.</p> <p>To summary, we will have approximately 13,251 data points available for training, and total number of images is around 53,004 with augmentation.</p> <h2 id="deep-learning-model-design">Deep Learning Model Design</h2> <p>To find the best fitting model, I experimented with 3 existing models:</p> <ul> <li><a href="http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf" rel="external nofollow noopener" target="_blank">NVIDIA Model</a></li> <li><a href="https://github.com/commaai/research/blob/master/train_steering_model.py" rel="external nofollow noopener" target="_blank">Comma.ai Model</a></li> <li><a href="https://arxiv.org/pdf/1409.1556.pdf" rel="external nofollow noopener" target="_blank">Simplified VGG Net - with configuration A</a></li> </ul> <p>All of three models I explored share the following design structure:</p> <ol> <li>First phrase: <ul> <li>Normalize input image data to -1 to 1 or -0.5 to 0.5 range</li> <li>Color space conversion layer(Optional): enable the model to select between 3 color channels</li> </ul> </li> <li>Second phrase: <ul> <li> <code class="language-plaintext highlighter-rouge">Convolution Layer</code> are applied with 5x5 filter size but the depth increases at each layer such as 24, 36, 48. Then, 2 convolution layers are applied with 3x3 filter size and 64 depth.</li> <li> <code class="language-plaintext highlighter-rouge">Maxpooling Layer</code> can also be used at choice to decrease the input size</li> <li> <code class="language-plaintext highlighter-rouge">ReLU/ELU Activation</code> is applied following every convolution layer</li> </ul> </li> <li>Third phrase: <ul> <li> <code class="language-plaintext highlighter-rouge">Dense Layer</code> – Output from previous layer are flatten. Then dense to 1 output (steering angle) via a few progressive layers, for example from 100 to 20 to 1. At each dense layer except the last, 10% to 50% (at your choice) Dropout is also applied to avoid overfitting. L2 weight regularization is recommended in every convolution and dense layer to produce a smoother driving performance. After many trial and error, 0.001 produce best peformance for this model.</li> <li> <code class="language-plaintext highlighter-rouge">Dropout Layer</code> – To avoid overfitting, Dropout with certain percentage can be added before or after the dense layer.</li> </ul> </li> <li>Fourth phrase: <ul> <li>Optimizer(Learning Rate): Adam optimizer is suitable for this project, which can automatically adjust the learning rate. It is set with default value 0.001, but 0.0001 learning rate is recommended to avoid too aggressive error descedent and produce a smoother ride. Therefore, 0.0001 learning rate is selected.</li> <li>Loss Function (or objective function, or optimization score function): Mean Squared Error(MSE) is used to measure the deviations of model predicted steering angle to human driver’s steering angle.</li> </ul> </li> </ol> <p>Structure summary of above three models implemented with Keras:</p> <h3 id="1-nvidia-model">1. NVIDIA Model</h3> <p>|Model | Model Layer Details | |————————-|————————-|</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model1-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model1_detail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model1_detail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model1_detail-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model1_detail.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="2-commaai-model">2. Comma.ai Model</h3> <p>|Model | Model Layer Details | |————————-|————————-|</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model2-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model2_detail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model2_detail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model2_detail-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model2_detail.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="3-simplified-vgg-net---with-configuration-a">3. Simplified VGG Net - with configuration A</h3> <p>|Model | Model Layer Details | |————————-|————————-|</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model3-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model3_detail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model3_detail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model3_detail-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model3_detail.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="model-training">Model Training</h2> <h3 id="image-crop">Image Crop</h3> <ol> <li>To help the system avoid learning other part of the image but only the track, we can crop the up part (sky) and bottom part (front part of the car deck) out of the image. Original image size (160x320), after cropping 60px on top and 20px on the bottom, and 10px from left and right, the new image size is (80x300).</li> <li>To help running a smaller training model, images are scaled to size (66x200) from cropped size (80x300).</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/process_image-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/process_image-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/process_image-1400.webp"></source> <img src="/al-folio/assets/img/project_4/process_image.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="convert-to-yuv-color-channels">Convert to YUV color channels</h3> <p>The input image is split into YUV planes and passed to the network (reference the <a href="http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf" rel="external nofollow noopener" target="_blank">NVIDIA paper</a>). <img src="assets/img/project_4/convert2YUV.png" alt="alt text"></p> <h3 id="training-and-validation">Training and Validation</h3> <p>Central images and steering angles are shuffle and split into 70/30 for Training/Validation using <code class="language-plaintext highlighter-rouge">shuffle</code> &amp; <code class="language-plaintext highlighter-rouge">train_test_split</code> from <code class="language-plaintext highlighter-rouge">sklearn</code>. Finally randomly shuffled the data set and put 30% of the data into a validation set.</p> <h3 id="recovery">Recovery</h3> <p>In general sense, driving behavior can be trained using the central images because we always look ahead when driving. Driving is mostly straight driving as well or small adjustment to turn the car unless there is a sharp turn. Below is the plot of steering angle on track 1 from Udacity data.</p> <p>But from inutition, if our car goes off lane (for example, distraction during text and drive), we can adjust it back on track right away. The machine doesn’t have this intuition, so once it goes off road it would be hard to recover. To teach the machine this kind of recovery knowledge, we have to show it the scenarios. Hence, we use left and right camera images. Udacity gives out a great hint how to apply this method.</p> <blockquote> <p>In the simulator, you can weave all over the road and turn recording on and off. In a real car, however, that’s not really possible. At least not legally. So in a real car, we’ll have multiple cameras on the vehicle, and we’ll map recovery paths from each camera. <strong>For example, if you train the model to associate a given image from the center camera with a left turn, then you could also train the model to associate the corresponding image from the left camera with a somewhat softer left turn. And you could train the model to associate the corresponding image from the right camera with an even harder left turn.</strong></p> </blockquote> <h3 id="steering-angle-correction">Steering Angle Correction</h3> <p>When we process the left and right camera, we add corrections (+0.25 or -0.25) for their steering angles because we only know the ground-truth steering angle for the center camera (as given by Udacity simulator). Therefore, it may introduce some small errors for the steering angles of left and right images.</p> <p>The chosen left/right images and adjusted angles are then added into driving left or driving right lists. Here is the logic: <strong><em>adjustment_angle = 0.25</em></strong></p> <ol> <li> <strong>Left turn</strong><em>(negative angle)</em>: + adjustment_angle on left image, - adjustment_angle on right image</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/left_turn-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/left_turn-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/left_turn-1400.webp"></source> <img src="/al-folio/assets/img/project_4/left_turn.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <strong>Right turn</strong><em>(positive angle)</em>: + adjustment_angle on left image, - adjustment_angle on right image</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/right_turn-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/right_turn-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/right_turn-1400.webp"></source> <img src="/al-folio/assets/img/project_4/right_turn.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="generators">Generators</h3> <p>There are two generators in this project. <strong>Training generator</strong> is to generate samples per batches to feed into fit_generator(). fit_generator() is used to fit the training model. At each batch, random samples are picked, applied augmentation and preprocessing . So training samples feeding into model is always different. <strong>Validation generator</strong> is also to feed random samples in batches for validation, unlike training generator, only central images are used here and only proprocessing is applied.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: center"># of Epoch</th> <th style="text-align: center">Batch Size</th> <th style="text-align: center">Average Time per Epoch(in second)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">NVIDIA</td> <td style="text-align: center">20</td> <td style="text-align: center">64</td> <td style="text-align: center">54</td> </tr> <tr> <td style="text-align: left">Comma.ai</td> <td style="text-align: center">20</td> <td style="text-align: center">64</td> <td style="text-align: center">61</td> </tr> <tr> <td style="text-align: left">Simplified VGG Net-A</td> <td style="text-align: center">20</td> <td style="text-align: center">32</td> <td style="text-align: center">102</td> </tr> </tbody> </table> <h2 id="performance--evaluation">Performance &amp; Evaluation</h2> <h3 id="1-nvidia-model-1">1. NVIDIA Model</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model1_track1.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model1_track1.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model1_track1.gif-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model1_track1.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://youtu.be/is49elVxbgc?list=PLQefmzG-uoN76per1Sdg7212nH9LSKpi3" rel="external nofollow noopener" target="_blank">Youtube track 1</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model1_track2.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model1_track2.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model1_track2.gif-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model1_track2.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://youtu.be/GGblJz5YFFM?list=PLQefmzG-uoN76per1Sdg7212nH9LSKpi3" rel="external nofollow noopener" target="_blank">Youtube track 2</a></p> <h3 id="2-commaai-model-1">2. Comma.ai Model</h3> <p>[Youtube link]</p> <h3 id="3-simplified-vgg-net---with-configuration-a-1">3. Simplified VGG Net - with configuration A</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model3_track1.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model3_track1.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model3_track1.gif-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model3_track1.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://youtu.be/iyrtscvcQV8?list=PLQefmzG-uoN76per1Sdg7212nH9LSKpi3" rel="external nofollow noopener" target="_blank">Youtube track 1</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/project_4/model3_track2.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/project_4/model3_track2.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/project_4/model3_track2.gif-1400.webp"></source> <img src="/al-folio/assets/img/project_4/model3_track2.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://youtu.be/IRmzAbo1C2E?list=PLQefmzG-uoN76per1Sdg7212nH9LSKpi3" rel="external nofollow noopener" target="_blank">Youtube track 2</a></p> <h2 id="discussion">Discussion</h2> <p>After tried different network architecture – Nvidia, Comma AI and VGG-A one, the Nvidia one achieves the best cost efficient performance. Its model size is small while relatively good generalizability. This is the beauty and benefit of Transfer learning in machine learning.</p> <p>The last proposed model by Dr. Junsheng Fu is derived from VGG and LeNet, which is more complex than LeNet but smaller than VGG. To avoid overfitting, he added tow dropout layers into the model and reduce the number of neurons in FC layers, which reduce both the train loss and validation loss.</p> <p>The udacity dataset is already smoothed, or at least is joystick or wheeled controller input. And the model clones them. It is perfecty matched the title “behavioral clone”. If you are good driver, it clones. If you are bad driver, it clones too. However most of the time, it is hard to figure out who is better driver, human or machine.</p> <p>I thought a smooth steering curve is better for the training, but it turn out not really the case. Over smoothed curve yield very aggressive turnning. Maybe it is a very good result, we just need fine tune the controller to handle it properly. Also, in the real world, most of the time, the steering wheel is in netural position. Train the machine not to over react is harder than keep moving.</p> <p>20 minutes to train around 13,000 66x200 size images with GPU(GTX 980m) is not bad. It turns out that using GPU is about 4 times faster than CPU. The bottle neck seems to be how many images(batch size) GPU can process at the same time. The large batch size will consume larger GPU memory, but leads to faster training time.</p> <h2 id="future-work">Future Work</h2> <p>This is the self driving car future we are looking for. We need overcome some obstacles:</p> <ul> <li>Better training data</li> <li>More compute power</li> <li>Standardize Network Architecture</li> <li>Try and error approach, and engineering approach</li> <li>Working with real environment data</li> </ul> <p>The simulator also provide real time steering_angle, throttle, speed and image feed. Therefore, it is possible to record new training set driving by the machine. Then train the machine again. After few generation, the machine driver will be better than human. I am going to explore more about the reinforcement learning.</p> <p>The second version of simulator is out with a more challenging unseen track. It will be beneficial to generalize a model that can succefully drive in both tracks.</p> <h2 id="reference">Reference</h2> <p>There are many online resources available and helpful for this project. Thank you everyone to share them to the world.</p> <ul> <li>https://github.com/JunshengFu/driving-behavioral-cloning</li> <li>https://medium.com/@mohankarthik/cloning-a-car-to-mimic-human-driving-5c2f7e8d8aff#.kot5rcn4b</li> <li>https://medium.com/@ksakmann/behavioral-cloning-make-a-car-drive-like-yourself-dc6021152713#.dttvi1ki4</li> <li>https://chatbotslife.com/learning-human-driving-behavior-using-nvidias-neural-network-model-and-image-augmentation-80399360efee#.ykemywxos</li> <li>https://github.com/upul/behavioral_cloning</li> <li>https://review.udacity.com/</li> <li>http://stackoverflow.com/questions/1756096/understanding-generators-in-python</li> <li>http://www.pyimagesearch.com/2015/10/05/opencv-gamma-correction/</li> <li><a href="https://arxiv.org/pdf/1604.07316.pdf" rel="external nofollow noopener" target="_blank">NVIDIA’s “End to End Learning for Self-Driving Cars” paper</a></li> <li>https://github.com/mvpcom/Udacity-CarND-Project-3</li> <li>https://github.com/karolmajek/BehavioralCloning-CarSteering</li> <li>https://github.com/commaai/research/blob/master/train_steering_model.py</li> <li>https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9</li> <li>https://github.com/vxy10/P3-BehaviorCloning</li> <li>https://github.com/ctsuu/Behavioral-Cloning</li> <li>https://github.com/ancabilloni/SDC-P3-BehavioralCloning/</li> <li>https://github.com/georgesung/behavioral_cloning</li> <li>and many many comments in slack channels.</li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Hao Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/al-folio/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js"></script> <script defer src="/al-folio/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>