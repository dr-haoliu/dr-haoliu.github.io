---
---

@string{aps = {American Physical Society,}}

@Article{Hou2025,
    author={Hou, Zhen and Liu, Hao and Bian, Jiang and He, Xing and Zhuang, Yan},
    title={Enhancing medical coding efficiency through domain-specific fine-tuned large language models},
    journal={npj Health Systems},
    year={2025},
    month={May},
    day={01},
    volume={2},
    number={1},
    pages={14},
    abstract={Medical coding is essential for healthcare operations yet remains predominantly manual, error-prone (up to 20{\%}), and costly (up to {\$}18.2 billion annually). Although large language models (LLMs) have shown promise in natural language processing, their application to medical coding has produced limited accuracy. In this study, we evaluated whether fine-tuning LLMs with specialized ICD-10 knowledge can automate code generation across clinical documentation. We adopted a two-phase approach: initial fine-tuning using 74,260 ICD-10 code--description pairs, followed by enhanced training to address linguistic and lexical variations. Evaluations using a proprietary model (GPT-4o mini) on a cloud platform and an open-source model (Llama) on local GPUs demonstrated that initial fine-tuning increased exact matching from <1{\%} to 97{\%}, while enhanced fine-tuning further improved performance in complex scenarios, with real-world clinical notes achieving 69.20{\%} exact match and 87.16{\%} category match. These findings indicate that domain-specific fine-tuned LLMs can reduce manual burdens and improve reliability.},
    issn={3005-1959},
    doi={10.1038/s44401-025-00018-3},
    url={https://doi.org/10.1038/s44401-025-00018-3},
    abbr = {npj Health Systems},
}


@inproceedings{kollapally2025a,
  author = {Zhou, Shuxin and Sen, Pritam and Liu, Hao and Dehkordi, Mahshad Koohi H and Perl, Yehoshua},
  title = {CFC Annotator: A Cluster-Focused Combination Algorithm for Annotating Electronic Health Records by Referencing Interface Terminology},
  year = {2025},
  month = {jan},
  language = {en},
  booktitle = {18th International Conference on Health Informatics (HEALTHINF 2025)},
  abbr = {HEALTHINF},
}

@inproceedings{kollapally2024a,
  author = {Kollapally, Navya Martin and Dehkordi, Mahshad Koohi H and Perl, Yehoshua and Geller, James and Deek, Fadi P and Liu, Hao and
     Keloth, Vipina K and Elhanan, Gai and Einstein, Andrew J. and Zhou, Shuxin},
  title = {Using Clinical Entity Recognition for Curating an Interface Terminology to Aid Fast Skimming Of EHRs},
  publisher = {IEEE},
  year = {2024},
  month = {december},
  language = {en},
  booktitle = {2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM2024)},
  abbr = {BIBM},
}

@inproceedings{kollapally2024a,
  author = {Dehkordi, Mahshad Koohi H and Zhou, Shuxin and Perl, Yehoshua and Deek, Fadi P and Einstein, Andrew J.
                and Elhanan, Gai and He, Zhe and Liu, Hao},
  title = {Enhancing Patient Comprehension: An Effective Sequential Prompting Approach to Simplifying EHRs Using LLMs},
  publisher = {IEEE},
  year = {2024},
  month = {december},
  language = {en},
  booktitle = {2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM2024)},
  abbr = {BIBM},
}


@inproceedings{yadav2024a,
  author = {Yadav, Jyoti and Varde, Aparna S. and Liu, Hao and Antoniou, George and Xie, Lei},
  title = {Audiovisual Multimodal Cough Data Analysis for Tuberculosis Detection},
  publisher = {IEEE},
  year = {2024},
  month = {july},
  language = {en},
  booktitle = {2024 IEEE International Conference on Information, Intelligence, Systems and Applications (IISA2024)},
  abbr = {IISA},
}


@INPROCEEDINGS{10614405,
  author={Gopeekrishnan, Anand and Arif, Shibbir Ahmed and Liu, Hao},
  booktitle={2024 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)},
  title={Demo: Accelerating Patient Screening for Clinical Trials using Large Language Model Prompting},
  year={2024},
  volume={},
  number={},
  pages={214-215},
  keywords={Accuracy;Databases;Large language models;MIMICs;Clinical trials;Vectors;Encoding;clinical trial;eligibility criteria;large language model;patient screening},
  doi={10.1109/CHASE60773.2024.00045},
  html={10.1109/CHASE60773.2024.00045},
  abbr = {IEEE/ACM},
  }

@inproceedings{yadav2024a,
  author = {Yadav, Jyoti and Varde, Aparna S. and Liu, Hao and Antoniou, George and Xie, Lei},
  title = {Audiovisual Multimodal Cough Data Analysis for Tuberculosis Detection},
  publisher = {IEEE},
  year = {2024},
  month = {july},
  language = {en},
  booktitle = {2024 IEEE International Conference on Information, Intelligence, Systems and Applications (IISA2024)},
  abbr = {IISA},
}

@ARTICLE{Liu2024-ea,
  title    = "EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes",
  author   = "Zhou, Yiliang and Newbury, Abigail and Zhang, Gongbo and Idnay, Betina and Liu, Hao and Weng, Chunhua and Peng, Yifan",
  abstract = "The fundamental process of evidence extraction and synthesis in evidence-based medicine involves extracting PICO (Population, Intervention, Comparison, and Outcome) elements from biomedical literature. However, Outcomes, being the most complex elements, are often neglected or oversimplified in existing benchmarks. To address this issue, we present EvidenceOutcomes, a novel, large, annotated corpus of clinically meaningful outcomes extracted from biomedical literature. We first developed a robust annotation guideline for extracting clinically meaningful outcomes from text through iteration and discussion with clinicians and Natural Language Processing experts. Then, three independent annotators annotated the Results and Conclusions sections of a randomly selected sample of 500 PubMed abstracts and 140 PubMed abstracts from the existing EBM-NLP corpus. This resulted in EvidenceOutcomes with high-quality annotations of an inter-rater agreement of 0.76. Additionally, our fine-tuned PubMedBERT model, applied to these 500 PubMed abstracts, achieved an F1-score of 0.69 at the entity level and 0.76 at the token level on the subset of 140 PubMed abstracts from the EBM-NLP corpus. EvidenceOutcomes can serve as a shared benchmark to develop and test future machine learning algorithms to extract clinically meaningful outcomes from biomedical abstracts.",
  journal  = "NeurIPS (in submission)",
  year     =  2024,
  month    =  jun,
  language = "en",
  abbr     = {NeurIPS},
}

@INPROCEEDINGS {Liu2024-sd,
  author = {Liu, Hao and Zhou, Shuxin and Chen, Zhehuan and Perl, Yehoshua and Wang, Jiayin},
  booktitle = {2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)},
  title = {Using Generative Large Language Models for Hierarchical Relationship Prediction in Medical Ontologies},
  year = {2024},
  volume = {},
  issn = {},
  pages = {248-256},
  abstract = {This study extends the exploration of ontology enrichment by evaluating the performance of various open-sourced Large Language Models (LLMs) on the task of predicting hierarchical relationships (IS-A) in medical ontologies including SNOMED CT Clinical Finding and Procedure hierarchies and the human Disease Ontology. With the previous finetuned BERT models for hierarchical relationship prediction as the baseline, we assessed eight open-source generative LLMs for the same task. We observed only three models, without finetuning, demonstrated comparable or superior performance compared to the baseline BERT -based models. The best performance model OpenChat achieved a macro average F1 score of 0.96 (0.95) on SNOMED CT Clinical Finding (Procedure) hierarchy, an increase over 7% from the baseline 0.89 (0.85). On human Disease Ontology, OpenChat excels with an F1 score of 0.91, outperforming the second-best performance model Vicuna (0.84). Notably, some LLMs prove unsuitable for hierarchical relationship prediction tasks or appliable for concept placement of medical ontologies. We also explored various prompt templates and ensemble techniques to uncover potential confounding factors in applying LLMs for IS-A relation predictions for medical ontologies.},
  keywords = {accuracy;large language models;medical services;ontologies;predictive models;task analysis;informatics},
  doi = {10.1109/ICHI61247.2024.00040},
  html = {https://doi.ieeecomputersociety.org/10.1109/ICHI61247.2024.00040},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  month = {jun},
  abbr     = {ICHI},
}

@ARTICLE{Liu2024-au,
  title    = "Retrieval Augmented Scientific Claim Verification",
  author   = "Liu, Hao and Soroush Ali and Nestor, Jordan G. and Park, Elizabeth and Idnay, Betina and Fang, Yilu and Pan, Jane and Liao, Stan and Bernard, Marguerite and Peng, Yifan and Weng, Chunhua",
  abstract = "Objective: To evaluate the veracity of a PICO-based claim against clinical trial literature on PubMed.
                Materials and Methods: We construct CoVERt, a new Covid VERification dataset that consists of COVID-19-related PICO-compatible claims accompanied by clinical trial abstracts that either support or refute the claim. We then develop CliVER, an end-to-end scientific Claim VERification system for CoVERt. CliVER automatically selects abstracts from the clinical trial literature containing rational sentences that Support or Refute a given PICO-based claim. We further introduce an ensemble of three state-of-the-art systems for label prediction.
                Results: We indexed 189,648 abstracts in PubMed between January 2010 to October 2021 as our clinical literature repository. The performance of CliVER was evaluated by verifying 19 claims from six disease domains by clinicians. CliVER achieved a precision of 79.0% for abstract retrieval, 67.4% for sentence selection, and 63.2% for label prediction, respectively. In the label prediction evaluation on CoVERt, CliVER achieved an F1 score of 0.92, where the ensemble of label prediction models outperforms each state-of-the-art by an absolute increase in F1 score from 3% to 11%.
                Conclusion: CliVER is a pioneering system that demonstrates the potential to automate PICO-based claim verification on clinical trial publications. We hope it can be leveraged to reduce the labor cost of evidence extraction for clinical research and improve the efficiency of claim verification using the massive medical literature.",
  journal  = "JAMIA Open",
  year     =  2024,
  month    =  jan,
  language = "en",
  abbr     = {JAMIAO},
  selected = {true},
  html     = {https://doi.org/10.1093/jamiaopen/ooae021},
}

@ARTICLE{Liu2023-de,
  title     = {Using Annotation for Computerized Support for Fast Skimming of Cardiology Electronic Health Record Notes},
  author    = {Dehkordi, Mahshad Koohi H and Einstein, Andrew J and Zhou, Shuxin and Elhanan, Gai and Perl, Yehoshua and Keloth, Vipina K and Geller, James and Liu, Hao},
  journal   = {2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  pages     = {4043--4050},
  year      = {2023},
  month     = {dec},
  language  = {en},
  abbr      = {BIBM},
  html      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10385289},
  keywords  = {Annotations;Terminology;Transfer learning;Training data;Medical services;Manuals;Organizations;EHR;annotation;cardiology;fast skimming},
  doi       = {https://doi.org/10.1109/BIBM58861.2023.10385289},
}

@ARTICLE{Liu2023-su,
    title    = {The Suitability of UMLS and SNOMED-CT for Encoding Outcome Concepts},
    author   = {Newbury, Abigail and Liu, Hao and Idnay, Betina and Weng, Chunhua},
    year     = {2023},
    month    = {aug},
    journal  = {J. Am. Med. Inform. Assoc.},
    language = {en},
    abbr     = {JAMIA},
    html     = {https://doi.org/10.1093/jamia/ocad161},
}

@ARTICLE{Liu2022-hw,
    title    = {How Good is ChatGPT for Medication Evidence Synthesis?},
    author   = {Liu, Hao and Peng, Yifan and Weng, Chunhua},
    year     = {2023},
    month    = {april},
    journal  = {Medical Informatics Europe},
    language = {en},
    abbr     = {MIE},
    html     = {https://doi.org/10.3233/shti230347},
}

@ARTICLE{Liu2022-up,
  title    = "Can Race-sensitive Biomedical Embeddings Improve Healthcare Predictive Models?",
  author   = "Liu, Hao and Moustafa-Fahmy, Nour and Ta, Casey
              and Weng, Chunhua",
  abstract = "This reproducibility study presents an algorithm to weigh in race distribution data of clinical research study samples when training biomedical embeddings. We extracted 12,864 PubMed abstracts published between January 1st, 2000 and January 1st, 2022 and weighed them based on the race distribution data extracted from their corresponding clinical trials registered on ClinicalTrials.gov. We trained Word2vec and BERT embeddings and evaluated their performance on predicting length of hospital stay (LHS) and intensive care unit (ICU) readmission using MIMIC-IV electronic health record data. We observed that models trained using race-sensitive embeddings do not consistently outperform the neutral embeddings ones when used for LHS prediction (with similar Mean Absolute Error 1.975 vs. 2.008) or ICU readmission prediction (with similar accuracy 74.61% vs. 75.17% and the same AUC 0.775), respectively. We conclude that demographic sensitive embeddings do not necessarily significantly improve the accuracy of health predictive models as previously reported in the literature.",
  journal  = "AMIA Informatics Summits",
  volume   =  2023,
  month    =  march,
  year     =  2023,
  language = "en",
  selected = {true},
  abbr     = {AMIA},
  html     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10283113/},
}

@ARTICLE{Liu2023-au,
  title    = "A Data-Driven Approach to Optimizing Clinical Study Eligibility Criteria",
  author   = "Fang, Yilu and Liu, Hao and Idnay, Betina and Marder, Karen and Weng, Chunhua",
  abstract = "Objective: To evaluate the veracity of a PICO-based claim against clinical trial literature on PubMed.
                Materials and Methods: We construct CoVERt, a new Covid VERification dataset that consists of COVID-19-related PICO-compatible claims accompanied by clinical trial abstracts that either support or refute the claim. We then develop CliVER, an end-to-end scientific Claim VERification system for CoVERt. CliVER automatically selects abstracts from the clinical trial literature containing rational sentences that Support or Refute a given PICO-based claim. We further introduce an ensemble of three state-of-the-art systems for label prediction.
                Results: We indexed 189,648 abstracts in PubMed between January 2010 to October 2021 as our clinical literature repository. The performance of CliVER was evaluated by verifying 19 claims from six disease domains by clinicians. CliVER achieved a precision of 79.0% for abstract retrieval, 67.4% for sentence selection, and 63.2% for label prediction, respectively. In the label prediction evaluation on CoVERt, CliVER achieved an F1 score of 0.92, where the ensemble of label prediction models outperforms each state-of-the-art by an absolute increase in F1 score from 3% to 11%.
                Conclusion: CliVER is a pioneering system that demonstrates the potential to automate PICO-based claim verification on clinical trial publications. We hope it can be leveraged to reduce the labor cost of evidence extraction for clinical research and improve the efficiency of claim verification using the massive medical literature.",
  journal  = "J. Biomed. Inform.",
  month    =  april,
  year     =  2023,
  language = "en",
  abbr     = {JBI},
  html     = {https://doi.org/10.1016/j.jbi.2023.104375},
}

@ARTICLE{Liu2022-kt,
  title    = "Ontology-based Categorization of Clinical Studies by Their Conditions",
  author   = "Liu, Hao and Carini, Simona and Chen, Zhehuan and Phillips Hey, Spencer and Sim, Ida and Weng, Chunhua",
  abstract = "OBJECTIVE: The free-text Condition data field in the
              ClinicalTrials.gov is not amenable to computational processes for
              retrieving, aggregating and visualizing clinical studies by
              condition categories. This paper contributes a method for
              automated ontology-based categorization of clinical studies by
              their conditions. MATERIALS AND METHODS: Our method first maps
              text entries in ClinicalTrials.gov's Condition field to standard
              condition concepts in the OMOP Common Data Model by using SNOMED
              CT as a reference ontology and using Usagi for concept
              normalization, followed by hierarchical traversal of the SNOMED
              ontology for concept expansion, ontology-driven condition
              categorization, and visualization. We compared the accuracy of
              this method to that of the MeSH-based method. RESULTS: We
              reviewed the 4,506 studies on Vivli.org categorized by our
              method. Condition terms of 4,501 (99.89\%) studies were
              successfully mapped to SNOMED CT concepts, and with a minimum
              concept mapping score threshold, 4,428 (98.27\%) studies were
              categorized into 31 predefined categories. When validating with
              manual categorization results on a random sample of 300 studies,
              our method achieved an estimated categorization accuracy of
              95.7\%, while the MeSH-based method had an accuracy of 85.0\%.
              CONCLUSION: We showed that categorizing clinical studies using
              their Condition terms with referencing to SNOMED CT achieved a
              better accuracy and coverage than using MeSH terms. The proposed
              ontology-driven condition categorization was useful to create
              accurate clinical study categorization that enables clinical
              researchers to aggregate evidence from a large number of clinical
              studies.",
  journal  = "J. Biomed. Inform.",
  volume   =  135,
  pages    = "104235",
  month    =  nov,
  year     =  2022,
  keywords = "Categorization; Clinical Study; Data Visualization; Ontology;
              SNOMED CT",
  language = "en",
  selected = {true},
  abbr = {JBI},
  issn = {1532-0464},
    doi = {https://doi.org/10.1016/j.jbi.2022.104235},
    html = {https://www.sciencedirect.com/science/article/pii/S1532046422002404},
    code = {https://github.com/dr-haoliu/ontology-based-clinical-study-categorization}
}

@ARTICLE{Liu2021-hf,
  title    = "A Knowledge Base of Clinical Trial Eligibility Criteria",
  author   = "Liu, Hao and Yuan, Chi and Butler, Alex and Sun, Yingcheng and
              Weng, Chunhua",
  abstract = "OBJECTIVE: We present the Clinical Trial Knowledge Base, a
              regularly updated knowledge base of discrete clinical trial
              eligibility criteria equipped with a web-based user interface for
              querying and aggregate analysis of common eligibility criteria.
              MATERIALS AND METHODS: We used a natural language processing
              (NLP) tool named Criteria2Query (Yuan et al., 2019) to transform
              free text clinical trial eligibility criteria from
              ClinicalTrials.gov into discrete criteria concepts and attributes
              encoded using the widely adopted Observational Medical Outcomes
              Partnership (OMOP) Common Data Model (CDM) and stored in a
              relational SQL database. A web application accessible via RESTful
              APIs was implemented to enable queries and visual aggregate
              analyses. We demonstrate CTKB's potential role in EHR phenotype
              knowledge engineering using ten validated phenotyping algorithms.
              RESULTS: At the time of writing, CTKB contained 87,504
              distinctive OMOP CDM standard concepts, including Condition
              (47.82\%), Drug (23.01\%), Procedure (13.73\%), Measurement
              (24.70\%) and Observation (5.28\%), with 34.78\% for inclusion
              criteria and 65.22\% for exclusion criteria, extracted from
              352,110 clinical trials. The average hit rate of criteria
              concepts in eMERGE phenotype algorithms is 77.56\%. CONCLUSION:
              CTKB is a novel comprehensive knowledge base of discrete
              eligibility criteria concepts with the potential to enable
              knowledge engineering for clinical trial cohort definition,
              clinical trial population representativeness assessment,
              electronical phenotyping, and data gap analyses for using
              electronic health records to support clinical trial recruitment.",
  journal  = "J. Biomed. Inform.",
  volume   =  117,
  pages    = "103771",
  month    =  may,
  year     =  2021,
  keywords = "Clinical Trial; Eligibility Criteria; Knowledge Base; Natural
              Language Processing",
  language = "en",
  selected = {true},
    abbr = {JBI},
    issn = {1532-0464},
    doi = {https://doi.org/10.1016/j.jbi.2021.103771},
    html = {https://www.sciencedirect.com/science/article/pii/S1532046421001003},
}

@ARTICLE{Lin2022-sw,
  title    = "A Sample Size Extractor for {RCT} Reports",
  author   = "Lin, Fengyang and Liu, Hao and Moon, Paul and Weng, Chunhua",
  abstract = "Sample size is an important indicator of the power of randomized
              controlled trials (RCTs). In this paper, we designed a total
              sample size extractor using a combination of syntactic and
              machine learning methods, and evaluated it on 300 Covid-19
              abstracts (Covid-Set) and 100 generic RCT abstracts
              (General-Set). To improve the performance, we applied transfer
              learning from a large public corpus of annotated abstracts. We
              achieved an average F1 score of 0.73 on the Covid-Set testing
              set, and 0.60 on the General-Set using exact matches. The F1
              scores for loose matches on both datasets were over 0.74.
              Compared with the state-of-the-art tool, our extractor reports
              total sample sizes directly and improved F1 scores by at least
              4\% without transfer learning. We demonstrated that transfer
              learning improved the sample size extraction accuracy and
              minimized human labor on annotations.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  290,
  pages    = "617--621",
  month    =  jun,
  year     =  2022,
  keywords = "Natural Language Processing; Randomized Controlled Trial; Sample Size",
  language = "en",
  html = {https://pubmed.ncbi.nlm.nih.gov/35673090/},
  abbr = {MedInfo},
}

@ARTICLE{Liu2022-lu,
  title    = "Evaluation of {Criteria2Query}: Towards Augmented Intelligence
              for Cohort Identification",
  author   = "Liu, Cong and Liu, Hao and Ta, Casey and Roger, James and Butler,
              Alex and Lee, Junghwan and Kim, Jaehyun and Shang, Ning and Weng,
              Chunhua",
  abstract = "Electronic healthcare records data promises to improve the
              efficiency of patient eligibility screening, which is an
              important factor in the success of clinical trials and
              observational studies. To bridge the sociotechnical gap in cohort
              identification by end-users, who are clinicians or researchers
              unfamiliar with underlying EHR databases, we previously developed
              a natural language query interface named Criteria2Query (C2Q)
              that automatically transforms free-text eligibility criteria to
              executable database queries. In this study, we present a
              comprehensive evaluation of C2Q to generate more actionable
              insights to inform the design and evaluation of future natural
              language user interfaces for clinical databases, towards the
              realization of Augmented Intelligence (AI) for clinical cohort
              definition via e-screening.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  290,
  pages    = "297--300",
  month    =  jun,
  year     =  2022,
  keywords = "Artificial Intelligence (AI); Clinical Trial; Natural Language
              Processing",
  language = "en",
  html = {https://pubmed.ncbi.nlm.nih.gov/35673021/},
  abbr={MedInfo},
}

@ARTICLE{Chen2021-uq,
  title    = "Potential Role of Clinical Trial Eligibility Criteria in
              Electronic Phenotyping",
  author   = "Chen, Zhehuan and Liu, Hao and Butler, Alex and Ostropolets, Anna
              and Weng, Chunhua",
  abstract = "2,719 distinctive phenotyping variables from 176 electronic
              phenotypes were compared with 57,150 distinctive clinical trial
              eligibility criteria concepts to assess the phenotype knowledge
              overlap between them. We observed a high percentage (69.5\%) of
              eMERGE phenotype features and a lower percentage (47.6\%) of
              OHDSI phenotype features matched to clinical trial eligibility
              criteria, possibly due to the relative emphasis on specificity
              for eMERGE phenotypes and the relative emphasis on sensitivity
              for OHDSI phenotypes. The study results show the potential of
              reusing clinical trial eligibility criteria for phenotyping
              feature selection and moderate benefits of using them for local
              cohort query implementation.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  281,
  pages    = "148--152",
  month    =  may,
  year     =  2021,
  keywords = "Clinical Trial Eligibility Criteria; Electronic Phenotyping",
  language = "en",
  html={https://pubmed.ncbi.nlm.nih.gov/34042723/},
  abbr={MedInfo},
}

@ARTICLE{Zheng2018-pt,
  title    = "Overlapping Complex Concepts Have More Commission Errors,
              Especially in Intensive Terminology Auditing",
  author   = "Zheng, Ling and Liu, Hao and Perl, Yehoshua and Geller, James and
              Ochs, Christopher and Case, James T",
  abstract = "SNOMED CT is a large, complex and widely-used terminology.
              Auditing is part of the life cycle of terminologies. A review of
              terminologies' content can identify two error categories:
              commission errors, such as an incorrect parent or attribute
              relationship, indicating errors in a concept's modeling, and
              omission errors, such as missing a parent or attribute
              relationship, representing incomplete modeling of a concept.
              According to our experience, terminology curators are mostly
              interested in commission errors. In recent years, a long-term
              remodeling project has addressed modeling issues in SNOMED CT's
              Infectious disease and Congenital disease subhierarchies. In this
              longitudinal study, we investigated a posteriori the efficacy of
              complex concepts, called overlapping concepts, to identify
              commission errors during intensive auditing periods and during
              maintenance periods over several releases. The algorithmic
              implication is that when auditing resources are scarce, a
              methodology of auditing first, or only, the overlapping concepts
              will obtain a higher auditing yield.",
  journal  = "AMIA Annu. Symp. Proc.",
  volume   =  2018,
  pages    = "1157--1166",
  month    =  dec,
  year     =  2018,
  language = "en",
  html = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371375/},
  abbr={AMIA},
}

@ARTICLE{Chen2022-ip,
  title    = "Representation and Normalization of Complex Interventions for
              Evidence Computing",
  author   = "Chen, Zhehuan and Liu, Hao and Liao, Stan and Bernard, Marguerite
              and Kang, Tian and Stewart, Latoya A and Weng, Chunhua",
  abstract = "Complex interventions are ubiquitous in healthcare. A lack of
              computational representations and information extraction
              solutions for complex interventions hinders accurate and
              efficient evidence synthesis. In this study, we manually
              annotated and analyzed 3,447 intervention snippets from 261
              randomized clinical trial (RCT) abstracts and developed a
              compositional representation for complex interventions, which
              captures the spatial, temporal and Boolean relations between
              intervention components, along with an intervention normalization
              pipeline that automates three tasks: (i) treatment entity
              extraction; (ii) intervention component relation extraction; and
              (iii) attribute extraction and association. 361 intervention
              snippets from 29 unseen abstracts were included to report on the
              performance of the evaluation. The average F-measure was 0.74 for
              treatment entity extraction on an exact match and 0.82 for
              attribute extraction. The F-measure for relation extraction of
              multi-component complex interventions was 0.90. 93\% of extracted
              attributes were correctly attributed to corresponding treatment
              entities.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  290,
  pages    = "592--596",
  month    =  jun,
  year     =  2022,
  keywords = "Knowledge representation; complex intervention; evidence-based
              medicine; natural language processing",
  language = "en",
  html = {https://pubmed.ncbi.nlm.nih.gov/35673085/},
  abbr={MedInfo},
}

@ARTICLE{Chen2022-kq,
  title    = "{Data-Driven} Modeling of Randomized Controlled Trial Outcomes",
  author   = "Chen, Zhehuan and Fang, Yilu and Liu, Hao and Weng, Chunhua",
  abstract = "Anecdotally, 38.5\% of clinical outcome descriptions in
              randomized controlled trial publications contain complex text.
              Existing terminologies are insufficient to standardize outcomes
              and their measures, temporal attributes, quantitative metrics,
              and other attributes. In this study, we analyzed the semantic
              patterns in the outcome text in a sample of COVID-19 trials and
              presented a data-driven method for modeling outcomes. We conclude
              that a data-driven knowledge representation can benefit natural
              language processing of outcome text from published clinical
              studies.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  294,
  pages    = "392--396",
  month    =  may,
  year     =  2022,
  keywords = "knowledge representation; outcome; randomized controlled trials",
  language = "en",
  html = {https://pubmed.ncbi.nlm.nih.gov/35612103/},
  abbr={MedInfo},
}

@ARTICLE{Turfah2022-ri,
  title    = "Extending {PICO} with Observation Normalization for Evidence
              Computing",
  author   = "Turfah, Ali and Liu, Hao and Stewart, Latoya A and Kang, Tian and
              Weng, Chunhua",
  abstract = "While the PICO framework is widely used by clinicians for
              clinical question formulation when querying the medical
              literature, it does not have the expressiveness to explicitly
              capture medical findings based on any standard. In addition,
              findings extracted from the literature are represented as
              free-text, which is not amenable to computation. This research
              extends the PICO framework with Observation elements, which
              capture the observed effect that an Intervention has on an
              Outcome, forming Intervention-Observation-Outcome triplets. In
              addition, we present a framework to normalize Observation
              elements with respect to their significance and the direction of
              the effect, as well as a rule-based approach to perform the
              normalization of these attributes. Our method achieves
              macro-averaged F1 scores of 0.82 and 0.73 for identifying the
              significance and direction attributes, respectively.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  290,
  pages    = "268--272",
  month    =  jun,
  year     =  2022,
  keywords = "Evidence-based Medicine; Natural Language Processing; Text Mining",
  language = "en",
  html = {https://pubmed.ncbi.nlm.nih.gov/35673015/},
  abbr={MedInfo},
}

@ARTICLE{Liu2017-ic,
  title    = "Correcting Ontology Errors Simplifies Visual Complexity",
  author   = "Liu, Hao and Zheng, Ling and Perl, Yehoshua and Chen, Yan and
              Elhanan, Gai",
  abstract = "In previous research we have shown that hierarchically complex
              overlapping concepts have a higher error rate of errors versus
              control concepts. In this poster we show an exmaple from Neoplasm
              concepts of the NCI thesaurus (NCIt) demonstrating that erroneous
              overplapping concepts, reflected in the partial-area units of a
              partial-area taxonomy, display visual complexity. Furthermore,
              correcting these erroneous concepts causes visual simplification.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  245,
  pages    = "1330",
  year     =  2017,
  keywords = "Ontology Quality Assurance; Ontology Visualization",
  language = "en",
  html = {https://pubmed.ncbi.nlm.nih.gov/29295411/},
  abbr={MedInfo},
}



@ARTICLE{Zheng2020-at,
  title    = "Missing Lateral Relationships in Top-level Concepts of an Ontology",
  author   = "Zheng, Ling and Chen, Yan and Min, Hua and Hildebrand, P Lloyd
              and Liu, Hao and Halper, Michael and Geller, James and de
              Coronado, Sherri and Perl, Yehoshua",
  abstract = "BACKGROUND: Ontologies house various kinds of domain knowledge in
              formal structures, primarily in the form of concepts and the
              associative relationships between them. Ontologies have become
              integral components of many health information processing
              environments. Hence, quality assurance of the conceptual content
              of any ontology is critical. Relationships are foundational to
              the definition of concepts. Missing relationship errors (i.e.,
              unintended omissions of important definitional relationships) can
              have a deleterious effect on the quality of an ontology. An
              abstraction network is a structure that overlays an ontology and
              provides an alternate, summarization view of its contents. One
              kind of abstraction network is called an area taxonomy, and a
              variation of it is called a subtaxonomy. A methodology based on
              these taxonomies for more readily finding missing relationship
              errors is explored. METHODS: The area taxonomy and the
              subtaxonomy are deployed to help reveal concepts that have a high
              likelihood of exhibiting missing relationship errors. A specific
              top-level grouping unit found within the area taxonomy and
              subtaxonomy, when deemed to be anomalous, is used as an indicator
              that missing relationship errors are likely to be found among
              certain concepts. Two hypotheses pertaining to the effectiveness
              of our Quality Assurance approach are studied. RESULTS: Our
              Quality Assurance methodology was applied to the Biological
              Process hierarchy of the National Cancer Institute thesaurus
              (NCIt) and SNOMED CT's Eye/vision finding subhierarchy within its
              Clinical finding hierarchy. Many missing relationship errors were
              discovered and confirmed in our analysis. For both test-bed
              hierarchies, our Quality Assurance methodology yielded a
              statistically significantly higher number of concepts with
              missing relationship errors in comparison to a control sample of
              concepts. Two hypotheses are confirmed by these findings.
              CONCLUSIONS: Quality assurance is a critical part of an
              ontology's lifecycle, and automated or semi-automated tools for
              supporting this process are invaluable. We introduced a Quality
              Assurance methodology targeted at missing relationship errors.
              Its successful application to the NCIt's Biological Process
              hierarchy and SNOMED CT's Eye/vision finding subhierarchy
              indicates that it can be a useful addition to the arsenal of
              tools available to ontology maintenance personnel.",
  journal  = "BMC Med. Inform. Decis. Mak.",
  volume   =  20,
  number   = "Suppl 10",
  pages    = "305",
  month    =  dec,
  year     =  2020,
  keywords = "Abstraction network; Error concentration; Missing relationship
              error; National Cancer Institute thesaurus (NCIt); Omission
              error; Ontology modeling; Ontology quality assurance; SNOMED CT;
              Taxonomy",
  html = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01319-3},
  language = "en",
  abbr = {BMC},
}

@ARTICLE{Fang2021-ku,
  title    = "Participatory Design of a Clinical Trial Eligibility Criteria Simplification Method",
  author   = "Fang, Yilu and Kim, Jae Hyun and Idnay, Betina Ross and Aragon
              Garcia, Rebeca and Castillo, Carmen E and Sun, Yingcheng and Liu,
              Hao and Liu, Cong and Yuan, Chi and Weng, Chunhua",
  abstract = "Clinical trial eligibility criteria are important for selecting
              the right participants for clinical trials. However, they are
              often complex and not computable. This paper presents the
              participatory design of a human-computer collaboration method for
              criteria simplification that includes natural language processing
              followed by user-centered eligibility criteria simplification. A
              case study on the ARCADIA trial shows how criteria were
              simplified for structured database querying by clinical
              researchers and identifies rules for criteria simplification and
              concept normalization.",
  journal  = "Stud. Health Technol. Inform.",
  volume   =  281,
  pages    = "984--988",
  month    =  may,
  year     =  2021,
  keywords = "concept mapping; intelligence augmentation; named entity
              recognition",
  language = "en",
  html = {https://pubmed.ncbi.nlm.nih.gov/34042820/},
  abbr={MedInfo},
}

@ARTICLE{Fang2022-iz,
  title    = "Combining Human and Machine Intelligence for Clinical Trial Eligibility Querying",
  author   = "Fang, Yilu and Idnay, Betina and Sun, Yingcheng and Liu, Hao and
              Chen, Zhehuan and Marder, Karen and Xu, Hua and Schnall, Rebecca
              and Weng, Chunhua",
  abstract = "OBJECTIVE: To combine machine efficiency and human intelligence
              for converting complex clinical trial eligibility criteria text
              into cohort queries. MATERIALS AND METHODS: Criteria2Query (C2Q)
              2.0 was developed to enable real-time user intervention for
              criteria selection and simplification, parsing error correction,
              and concept mapping. The accuracy, precision, recall, and F1
              score of enhanced modules for negation scope detection, temporal
              and value normalization were evaluated using a previously curated
              gold standard, the annotated eligibility criteria of 1010
              COVID-19 clinical trials. The usability and usefulness were
              evaluated by 10 research coordinators in a task-oriented
              usability evaluation using 5 Alzheimer's disease trials. Data
              were collected by user interaction logging, a demographic
              questionnaire, the Health Information Technology Usability
              Evaluation Scale (Health-ITUES), and a feature-specific
              questionnaire. RESULTS: The accuracies of negation scope
              detection, temporal and value normalization were 0.924, 0.916,
              and 0.966, respectively. C2Q 2.0 achieved a moderate usability
              score (3.84 out of 5) and a high learnability score (4.54 out of
              5). On average, 9.9 modifications were made for a clinical study.
              Experienced researchers made more modifications than novice
              researchers. The most frequent modification was deletion (5.35
              per study). Furthermore, the evaluators favored cohort queries
              resulting from modifications (score 4.1 out of 5) and the user
              engagement features (score 4.3 out of 5). DISCUSSION AND
              CONCLUSION: Features to engage domain experts and to overcome the
              limitations in automated machine output are shown to be useful
              and user-friendly. We concluded that human-computer collaboration
              is key to improving the adoption and user-friendliness of natural
              language processing.",
  journal  = "J. Am. Med. Inform. Assoc.",
  volume   =  29,
  number   =  7,
  pages    = "1161--1171",
  month    =  jun,
  year     =  2022,
  keywords = "cohort identification; eligibility prescreening; human--computer
              collaboration; informatics",
  language = "en",
  html = {https://academic.oup.com/jamia/article/29/7/1161/6569054},
  abbr={JAMIA},
}

@ARTICLE{Franks2021-pk,
  title    = "Misalignment Between {COVID-19} Hotspots and Clinical Trial Sites",
  author   = "Franks, Lauren and Liu, Hao and Elkind, Mitchell S V and Reilly,
              Muredach P and Weng, Chunhua and Lee, Shing M",
  abstract = "Hundreds of interventional clinical trials have been launched in
              the United States to identify effective treatment strategies for
              combating the coronavirus disease 2019 (COVID-19) pandemic.
              However, to date, only a small fraction of these trials have
              completed enrollment, delaying the scientific investigation of
              COVID-19 and its treatment options. This study presents novel
              metrics to examine the geographic alignment between COVID-19
              hotspots and interventional clinical trial sites and evaluate
              trial access over time during the evolving pandemic. Using
              temporal COVID-19 case data from USAFacts.org and trial data from
              ClinicalTrials.gov, U.S. counties were categorized based on their
              numbers of cases and trials. Our analysis suggests that alignment
              and access have worsened as the pandemic shifted over time. We
              recommend strategies and metrics to evaluate the alignment
              between cases and trials. Future studies are warranted to
              investigate the impact of the misalignment of cases and clinical
              trial sites on clinical trial recruitment.",
  journal  = "J. Am. Med. Inform. Assoc.",
  volume   =  28,
  number   =  11,
  pages    = "2461--2466",
  month    =  oct,
  year     =  2021,
  keywords = "COVID-19; clinical; clinical research informatics; clinical trial
              site location; trial recruitment",
  language = "en",
  html = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8385938/},
  abbr={JAMIA},
}


@ARTICLE{Liu2014-xf,
  title     = "Microblogging as a Social Sensing Tool",
  author    = "Liu, Hao and Dong, Ziqian and Gu, Huanying",
  abstract  = "Microblogging has become a popular communication tool among
               Internet users. Millions of users retrieve information and share
               opinions on different aspects of their daily lives using …",
  journal   = "11th IEEE International Conference on Networking, Sensing and Control",
  publisher = "ieeexplore.ieee.org",
  year      =  2014,
  abbr={IEEE},
}


@ARTICLE{Zheng2017-en,
  title     = "Multi-layer Big Knowledge Visualization Scheme for Comprehending
               Neoplasm Ontology Content",
  author    = "Zheng, Ling and Ochs, Christopher and Geller, James and Liu, Hao and Perl, Yehoshua and
               de Coronado, Sherri",
  abstract  = "Big Knowledge repositories, in the form of large ontologies,
               typically consist of many thousands of knowledge assertions.
               They have complex network structures consisting of …",
  journal   = "IEEE International Conference on Big Knowledge (ICBK)",
  publisher = "ieeexplore.ieee.org",
  year      =  2017,
  abbr={IEEE},
}

@ARTICLE{Zheng2021-wa,
  title    = "Visual Comprehension and Orientation into the {COVID-19} {CIDO}
              Ontology",
  author   = "Zheng, Ling and Perl, Yehoshua and He, Yongqun and Ochs,
              Christopher and Geller, James and Liu, Hao and Keloth, Vipina K",
  abstract = "The current intensive research on potential remedies and
              vaccinations for COVID-19 would greatly benefit from an ontology
              of standardized COVID terms. The Coronavirus Infectious Disease
              Ontology (CIDO) is the largest among several COVID ontologies,
              and it keeps growing, but it is still a medium sized ontology.
              Sophisticated CIDO users, who need more than searching for a
              specific concept, require orientation and comprehension of CIDO.
              In previous research, we designed a summarization network called
              ``partial-area taxonomy'' to support comprehension of ontologies.
              The partial-area taxonomy for CIDO is of smaller magnitude than
              CIDO, but is still too large for comprehension. We present here
              the ``weighted aggregate taxonomy'' of CIDO, designed to provide
              compact views at various granularities of our partial-area
              taxonomy (and the CIDO ontology). Such a compact view provides a
              ``big picture'' of the content of an ontology. In previous work,
              in the visualization patterns used for partial-area taxonomies,
              the nodes were arranged in levels according to the numbers of
              relationships of their concepts. Applying this visualization
              pattern to CIDO's weighted aggregate taxonomy resulted in an
              overly long and narrow layout that does not support orientation
              and comprehension since the names of nodes are barely readable.
              Thus, we introduce in this paper an innovative visualization of
              the weighted aggregate taxonomy for better orientation and
              comprehension of CIDO (and other ontologies). A measure for the
              efficiency of a layout is introduced and is used to demonstrate
              the advantage of the new layout over the previous one. With this
              new visualization, the user can ``see the forest for the trees''
              of the ontology. Benefits of this visualization in highlighting
              insights into CIDO's content are provided. Generality of the new
              layout is demonstrated.",
  journal  = "J. Biomed. Inform.",
  volume   =  120,
  pages    = "103861",
  month    =  aug,
  year     =  2021,
  keywords = "Comprehension of an ontology; Coronavirus Infectious Disease
              Ontology; Ontology orientation; Ontology summarization; Ontology
              visualization; Summarization networks",
  language = "en",
  abbr={JBI},
}


@ARTICLE{Liu_undated-fy,
  title    = "A Quality Assurance Methodology for {ChEBI} Ontology Focusing on
              Uncommonly Modeled Concepts",
  author   = {Liu, Hao and Chen, Ling and Zheng, Ling and Perl, Yehoshua and Geller, James},
  abstract = "The Chemical Entities of Biological Interest (ChEBI) ontology is
              an important knowledge source of chemical entities in a
              biological context. ChEBI is large and complex, making it …",
  journal  = "International Conference on Biomedical Ontology (ICBO)",
  year     = 2018,
  abbr={ICBO},
}

@ARTICLE{Sun2021-vi,
  title    = "A Framework for Systematic Assessment of Clinical Trial
              Population Representativeness Using Electronic Health Records
              Data",
  author   = "Sun, Yingcheng and Butler, Alex and Diallo, Ibrahim and Kim, Jae
              Hyun and Ta, Casey and Rogers, James R and Liu, Hao and Weng,
              Chunhua",
  abstract = "BACKGROUND: Clinical trials are the gold standard for generating
              robust medical evidence, but clinical trial results often raise
              generalizability concerns, which can be attributed to the lack of
              population representativeness. The electronic health records
              (EHRs) data are useful for estimating the population
              representativeness of clinical trial study population.
              OBJECTIVES: This research aims to estimate the population
              representativeness of clinical trials systematically using EHR
              data during the early design stage. METHODS: We present an
              end-to-end analytical framework for transforming free-text
              clinical trial eligibility criteria into executable database
              queries conformant with the Observational Medical Outcomes
              Partnership Common Data Model and for systematically quantifying
              the population representativeness for each clinical trial.
              RESULTS: We calculated the population representativeness of 782
              novel coronavirus disease 2019 (COVID-19) trials and 3,827 type 2
              diabetes mellitus (T2DM) trials in the United States respectively
              using this framework. With the use of overly restrictive
              eligibility criteria, 85.7\% of the COVID-19 trials and 30.1\% of
              T2DM trials had poor population representativeness. CONCLUSION:
              This research demonstrates the potential of using the EHR data to
              assess the clinical trials population representativeness,
              providing data-driven metrics to inform the selection and
              optimization of eligibility criteria.",
  journal  = "Appl. Clin. Inform.",
  volume   =  12,
  number   =  4,
  pages    = "816--825",
  month    =  aug,
  year     =  2021,
  language = "en",
  abbr={ACI},
}

@ARTICLE{Li2021-up,
  title    = "A Comparison between Human and {NLP-based} Annotation of Clinical
              Trial Eligibility Criteria Text Using The {OMOP} Common Data
              Model",
  author   = "Li, Xinhang and Liu, Hao and Kury, Fabr{\'\i}cio and Yuan, Chi
              and Butler, Alex and Sun, Yingcheng and Ostropolets, Anna and Xu,
              Hua and Weng, Chunhua",
  abstract = "Human annotations are the established gold standard for
              evaluating natural language processing (NLP) methods. The goals
              of this study are to quantify and qualify the disagreement
              between human and NLP. We developed an NLP system for annotating
              clinical trial eligibility criteria text and constructed a
              manually annotated corpus, both following the OMOP Common Data
              Model (CDM). We analyzed the discrepancies between the human and
              NLP annotations and their causes (e.g., ambiguities in concept
              categorization and tacit decisions on inclusion of qualifiers and
              temporal attributes during concept annotation). This study
              initially reported complexities in clinical trial eligibility
              criteria text that complicate NLP and the limitations of the OMOP
              CDM. The disagreement between and human and NLP annotations may
              be generalizable. We discuss implications for NLP evaluation.",
  journal  = "AMIA Jt Summits Transl Sci Proc",
  volume   =  2021,
  pages    = "394--403",
  month    =  may,
  year     =  2021,
  language = "en",
  html = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8378608/},
  abbr={AMIA},
}

@ARTICLE{Sun2021-lx,
  title    = "The {COVID-19} Trial Finder",
  author   = "Sun, Yingcheng and Butler, Alex and Lin, Fengyang and Liu, Hao
              and Stewart, Latoya A and Kim, Jae Hyun and Idnay, Betina Ross S
              and Ge, Qingyin and Wei, Xinyi and Liu, Cong and Yuan, Chi and
              Weng, Chunhua",
  abstract = "Clinical trials are the gold standard for generating reliable
              medical evidence. The biggest bottleneck in clinical trials is
              recruitment. To facilitate recruitment, tools for patient search
              of relevant clinical trials have been developed, but users often
              suffer from information overload. With nearly 700 coronavirus
              disease 2019 (COVID-19) trials conducted in the United States as
              of August 2020, it is imperative to enable rapid recruitment to
              these studies. The COVID-19 Trial Finder was designed to
              facilitate patient-centered search of COVID-19 trials, first by
              location and radius distance from trial sites, and then by brief,
              dynamically generated medical questions to allow users to
              prescreen their eligibility for nearby COVID-19 trials with
              minimum human computer interaction. A simulation study using 20
              publicly available patient case reports demonstrates its
              precision and effectiveness.",
  journal  = "J. Am. Med. Inform. Assoc.",
  volume   =  28,
  number   =  3,
  pages    = "616--621",
  month    =  mar,
  year     =  2021,
  keywords = "COVID-19; clinical trial; eligibility criteria; information
              filtering; questionnaire; web application",
  language = "en",
  html     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7717322/},
  abbr     = {JAMIA},
}


@ARTICLE{Liu2018-ar,
  title     = "Enrichment of {SNOMED} {CT} Ophthalmology Component to Support
               {EHR} Coding",
  author    = {Liu, Hao and Hildebrand, P Lloyd and Perl, Yehoshua and Geller, James},
  abstract  = "The US government has offered major financial incentives to
               encourage MDs, including ophthalmologists, to adopt Electronic
               Health Records (EHRs) in their practices. SNOMED …",
  journal   = "2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
  publisher = "ieeexplore.ieee.org",
  year      =  2018,
  abbr      = {IEEE BIBM},
  pages     = {1990--1997},
  organization = {IEEE},
}

@ARTICLE{Sun2021-dv,
  title    = "Building an {OMOP} Common Data Model-compliant Annotated Corpus
              for {COVID-19} Clinical Trials",
  author   = "Sun, Yingcheng and Butler, Alex and Stewart, Latoya A and Liu,
              Hao and Yuan, Chi and Southard, Christopher T and Kim, Jae Hyun
              and Weng, Chunhua",
  abstract = "Clinical trials are essential for generating reliable medical
              evidence, but often suffer from expensive and delayed patient
              recruitment because the unstructured eligibility criteria
              description prevents automatic query generation for eligibility
              screening. In response to the COVID-19 pandemic, many trials have
              been created but their information is not computable. We included
              700 COVID-19 trials available at the point of study and developed
              a semi-automatic approach to generate an annotated corpus for
              COVID-19 clinical trial eligibility criteria called COVIC. A
              hierarchical annotation schema based on the OMOP Common Data
              Model was developed to accommodate four levels of annotation
              granularity: i.e., study cohort, eligibility criteria, named
              entity and standard concept. In COVIC, 39 trials with more than
              one study cohorts were identified and labelled with an identifier
              for each cohort. 1,943 criteria for non-clinical characteristics
              such as ``informed consent'', ``exclusivity of participation''
              were annotated. 9767 criteria were represented by 18,161 entities
              in 8 domains, 7,743 attributes of 7 attribute types and 16,443
              relationships of 11 relationship types. 17,171 entities were
              mapped to standard medical concepts and 1,009 attributes were
              normalized into computable representations. COVIC can serve as a
              corpus indexed by semantic tags for COVID-19 trial search and
              analytics, and a benchmark for machine learning based criteria
              extraction.",
  journal  = "J. Biomed. Inform.",
  volume   =  118,
  pages    = "103790",
  month    =  jun,
  year     =  2021,
  keywords = "COVID-19; Clinical trial; Eligibility criteria; Machine readable
              dataset; Structured text corpus",
  language = "en",
  abbr={JBI},
}

@ARTICLE{Zheng2019-gi,
  title    = "Training a Convolutional Neural Network with Terminology
              Summarization Data Improves {SNOMED} {CT} Enrichment",
  author   = "Zheng, Ling and Liu, Hao and Perl, Yehoshua and Geller, James",
  abstract = "As a step toward learning to automatically insert new concepts
              into a large biomedical ontology, we are studying the easier
              problem of automatically verifying that an IS-A link should exist
              between a new child concept and an existing parent concept. We
              are using a Convolutional Neural Network, a powerful machine
              learning method. However, results depend on the quality of the
              training data. We use SNOMED CT (July 2017) for training and the
              subsequent release for testing. The main problem is to find a
              good set of negative training data. We experiment with two
              approaches, based on uncle-nephew (not connected) pairs of
              concepts. We contrast using the complete Clinical Finding
              hierarchy of SNOMED CT with using the powerful Area Taxonomy
              ontology summarization mechanism to constrain the training data.
              The results for the task of verifying IS-A links are improved by
              8.6\% when going from the complete hierarchy to the Area
              Taxonomy.",
  journal  = "AMIA Annu. Symp. Proc.",
  volume   =  2019,
  pages    = "972--981",
  year     =  2019,
  language = "en",
  html = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7153126/},
  abbr={AMIA},
}

@ARTICLE{Liu2020-kh,
  title    = "Concept Placement using {BERT} Trained by Transforming and
              Summarizing Biomedical Ontology Structure",
  author   = "Liu, Hao and Perl, Yehoshua and Geller, James",
  abstract = "The comprehensive modeling and hierarchical positioning of a new
              concept in an ontology heavily relies on its set of proper
              subsumption relationships (IS-As) to other concepts. Identifying
              a concept's IS-A relationships is a laborious task requiring
              curators to have both domain knowledge and terminology skills. In
              this work, we propose a method to automatically predict the
              presence of IS-A relationships between a new concept and
              pre-existing concepts based on the language representation model
              BERT. This method converts the neighborhood network of a concept
              into ``sentences'' and harnesses BERT's Next Sentence Prediction
              (NSP) capability of predicting the adjacency of two sentences. To
              augment our method's performance, we refined the training data by
              employing an ontology summarization technique. We trained our
              model with the two largest hierarchies of the SNOMED CT 2017 July
              release and applied it to predicting the parents of new concepts
              added in the SNOMED CT 2018 January release. The results showed
              that our method achieved an average F1 score of 0.88, and the
              average Recall score improves slightly from 0.94 to 0.96 by using
              the ontology summarization technique.",
  journal  = "J. Biomed. Inform.",
  volume   =  112,
  pages    = "103607",
  month    =  dec,
  year     =  2020,
  keywords = "BERT; Machine learning; Natural language processing; Ontology
              placement; Ontology summarization; SNOMED CT",
  language = "en",
  selected = {true},
  html = {https://www.sciencedirect.com/science/article/pii/S1532046420302355},
  abbr={JBI},
}

@ARTICLE{Elhanan2017-dq,
  title    = "From {SNOMED} {CT} to Uberon: Transferability of Evaluation
              Methodology between Similarly Structured Ontologies",
  author   = "Elhanan, Gai and Ochs, Christopher and Mejino, Jr, Jose L V and
              Liu, Hao and Mungall, Christopher J and Perl, Yehoshua",
  abstract = "OBJECTIVE: To examine whether disjoint partial-area taxonomy, a
              semantically-based evaluation methodology that has been
              successfully tested in SNOMED CT, will perform with similar
              effectiveness on Uberon, an anatomical ontology that belongs to a
              structurally similar family of ontologies as SNOMED CT. METHOD: A
              disjoint partial-area taxonomy was generated for Uberon. One
              hundred randomly selected test concepts that overlap between
              partial-areas were matched to a same size control sample of
              non-overlapping concepts. The samples were blindly inspected for
              non-critical issues and presumptive errors first by a general
              domain expert whose results were then confirmed or rejected by a
              highly experienced anatomical ontology domain expert. Reported
              issues were subsequently reviewed by Uberon's curators. RESULTS:
              Overlapping concepts in Uberon's disjoint partial-area taxonomy
              exhibited a significantly higher rate of all issues. Clear-cut
              presumptive errors trended similarly but did not reach
              statistical significance. A sub-analysis of overlapping concepts
              with three or more relationship types indicated a much higher
              rate of issues. CONCLUSIONS: Overlapping concepts from Uberon's
              disjoint abstraction network are quite likely (up to 28.9\%) to
              exhibit issues. The results suggest that the methodology can
              transfer well between same family ontologies. Although Uberon
              exhibited relatively few overlapping concepts, the methodology
              can be combined with other semantic indicators to expand the
              process to other concepts within the ontology that will generate
              high yields of discovered issues.",
  journal  = "Artif. Intell. Med.",
  volume   =  79,
  pages    = "9--14",
  month    =  jun,
  year     =  2017,
  keywords = "Anatomy ontology; Disjoint abstraction network; Overlapping
              concepts; Quality assurance; Semantic complexity",
  language = "en",
  abbr={AIM},
}


@ARTICLE{Liu_undated-sn,
  title    = "Can a Convolutional Neural Network Support Auditing of NCI
              Thesaurus Neoplasm Concepts?",
  author   = {Liu, Hao and Zheng, Ling and Perl, Yehoshua and Geller, James and Elhanan, Gai},
  abstract = "Paper Title (use style: paper title) Page 1 Can a Convolutional
              Neural Network Support Auditing of NCI Thesaurus Neoplasm
              Concepts? Hao Liu1, Ling Zheng2, Yehoshua Perl1, James …",
  journal  = "International Conference on Biomedical Ontology (ICBO)",
  year     = 2018,
  abbr={ICBO},
}

@ARTICLE{Liu2018-al,
  title    = "Using Convolutional Neural Networks to Support Insertion of New
              Concepts into {SNOMED} {CT}",
  author   = "Liu, Hao and Geller, James and Halper, Michael and Perl, Yehoshua",
  abstract = "Many major medical ontologies go through a regular (bi-annual,
              monthly, etc.) release cycle. A new release will contain
              corrections to the previous release, as well as genuinely new
              concepts that are the result of either user requests or new
              developments in the domain. New concepts need to be placed at the
              correct place in the ontology hierarchy. Traditionally, this is
              done by an expert modeling a new concept and running a classifier
              algorithm. We propose an alternative approach that is based on
              providing only the name of a new concept and using a
              Convolutional Neural Network-based machine learning method. We
              first tested this approach within one version of SNOMED CT and
              achieved an average 88.5\% precision and an F1 score of 0.793. In
              comparing the July 2017 release with the January 2018 release,
              limiting ourselves to predicting one out of two or more parents,
              our average F1 score was 0.701.",
  journal  = "AMIA Annu. Symp. Proc.",
  volume   =  2018,
  pages    = "750--759",
  month    =  dec,
  year     =  2018,
  language = "en",
  html = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371320/},
    abbr   = {AMIA},
}



@ARTICLE{Liu2019-zi,
  title    = "Transfer Learning from {BERT} to Support Insertion of New
              Concepts into {SNOMED} {CT}",
  author   = "Liu, Hao and Perl, Yehoshua and Geller, James",
  abstract = "With advances in Machine Learning (ML), neural network-based
              methods, such as Convolutional/Recurrent Neural Networks, have
              been proposed to assist terminology curators in the development
              and maintenance of terminologies. Bidirectional Encoder
              Representations from Transformers (BERT), a new language
              representation model, obtains state-of-the-art results on a wide
              array of general English NLP tasks. We explore BERT's
              applicability to medical terminology-related tasks. Utilizing the
              ``next sentence prediction'' capability of BERT, we show that the
              Fine-tuning strategy of Transfer Learning (TL) from the BERTBASE
              model can address a challenging problem in automatic terminology
              enrichment - insertion of new concepts. Adding a pre-training
              strategy enhances the results. We apply our strategies to the two
              largest hierarchies of SNOMED CT, with one release as training
              data and the following release as test data. The performance of
              the combined two proposed TL models achieves an average F1 score
              of 0.85 and 0.86 for the two hierarchies, respectively.",
  journal  = "AMIA Annu. Symp. Proc.",
  volume   =  2019,
  pages    = "1129--1138",
  year     =  2019,
  language = "en",
  html = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7153142/},
    abbr={AMIA},
}

@ARTICLE{Kim2021-cn,
  title    = "Towards Clinical Data-driven Eligibility Criteria Optimization
              for Interventional {COVID-19} Clinical Trials",
  author   = "Kim, Jae Hyun and Ta, Casey N and Liu, Cong and Sung, Cynthia and
              Butler, Alex M and Stewart, Latoya A and Ena, Lyudmila and
              Rogers, James R and Lee, Junghwan and Ostropolets, Anna and Ryan,
              Patrick B and Liu, Hao and Lee, Shing M and Elkind, Mitchell S V
              and Weng, Chunhua",
  abstract = "OBJECTIVE: This research aims to evaluate the impact of
              eligibility criteria on recruitment and observable clinical
              outcomes of COVID-19 clinical trials using electronic health
              record (EHR) data. MATERIALS AND METHODS: On June 18, 2020, we
              identified frequently used eligibility criteria from all the
              interventional COVID-19 trials in ClinicalTrials.gov (n = 288),
              including age, pregnancy, oxygen saturation, alanine/aspartate
              aminotransferase, platelets, and estimated glomerular filtration
              rate. We applied the frequently used criteria to the EHR data of
              COVID-19 patients in Columbia University Irving Medical Center
              (CUIMC) (March 2020-June 2020) and evaluated their impact on
              patient accrual and the occurrence of a composite endpoint of
              mechanical ventilation, tracheostomy, and in-hospital death.
              RESULTS: There were 3251 patients diagnosed with COVID-19 from
              the CUIMC EHR included in the analysis. The median follow-up
              period was 10 days (interquartile range 4-28 days). The composite
              events occurred in 18.1\% (n = 587) of the COVID-19 cohort during
              the follow-up. In a hypothetical trial with common eligibility
              criteria, 33.6\% (690/2051) were eligible among patients with
              evaluable data and 22.2\% (153/690) had the composite event.
              DISCUSSION: By adjusting the thresholds of common eligibility
              criteria based on the characteristics of COVID-19 patients, we
              could observe more composite events from fewer patients.
              CONCLUSIONS: This research demonstrated the potential of using
              the EHR data of COVID-19 patients to inform the selection of
              eligibility criteria and their thresholds, supporting data-driven
              optimization of participant selection towards improved
              statistical power of COVID-19 trials.",
  journal  = "J. Am. Med. Inform. Assoc.",
  volume   =  28,
  number   =  1,
  pages    = "14--22",
  month    =  jan,
  year     =  2021,
  keywords = "COVID-19, criteria optimization ; clinical trial; eligibility
              criteria; real-world data",
  language = "en",
  html = {https://academic.oup.com/jamia/article/28/1/14/6015812},
    abbr={JAMIA},
}

@ARTICLE{Kury2020-mk,
  title    = "Chia, A Large Annotated Corpus of Clinical Trial Eligibility
              Criteria",
  author   = "Kury, Fabr{\'\i}cio and Butler, Alex and Yuan, Chi and Fu,
              Li-Heng and Sun, Yingcheng and Liu, Hao and Sim, Ida and Carini,
              Simona and Weng, Chunhua",
  abstract = "We present Chia, a novel, large annotated corpus of patient
              eligibility criteria extracted from 1,000 interventional, Phase
              IV clinical trials registered in ClinicalTrials.gov. This dataset
              includes 12,409 annotated eligibility criteria, represented by
              41,487 distinctive entities of 15 entity types and 25,017
              relationships of 12 relationship types. Each criterion is
              represented as a directed acyclic graph, which can be easily
              transformed into Boolean logic to form a database query. Chia can
              serve as a shared benchmark to develop and test future machine
              learning, rule-based, or hybrid methods for information
              extraction from free-text clinical trial eligibility criteria.",
  journal  = "Scientific Data",
  volume   =  7,
  number   =  1,
  pages    = "281",
  month    =  aug,
  year     =  2020,
  language = "en",
  html = {https://www.nature.com/articles/s41597-020-00620-0},
  abbr={SciData},
}


